---
title:   ISOS ML Privacy
context: ISOS201
author:  Huxley
source:  #index
---

#ref #ret 

---

%%- n: usa foreing intel survelance
- j: edward snowden, apple data collection. consumer notification!
- d: digital polical ads + tracking
- m: social media taking data in general
- h: human ml modeling
%%

# Page o' notes

### Machine learning broad level implications on privacy

- ML requires big data
	- machine learning has driven us into the "information age"
		- cause of most of these issues
	- getting lots of info -> invading privacy, generally
	
- ML can break privacy without explicit data
	- ML can de-anonymize anonymous activities
		- ML based tracking systems make anonymity hard
		- hashing ISN'T good enough
	- same thing can be done even without ml
		- location tracking easily de-anonymized by simple cross reference
		- 
- ML can construct human models
	- fundamentally about simulation intelligent behavior
	- ML model of how you think and act
		- viewed as creepy
	- if it could predict your thoughts, what then?

### Regulation!

- FTC (federal trade commission) coming down hard
	- hitting Facebook, google, ect.
- CCPA, California consumer privacy act
- GDPR, General Data Protection Regulation
- 

### Rights

- right to be forgotten
	- opt out law by the consumer
	- is this a right?
	
- right to our data, as outlined in CCPA
	- again, really a right?
	
- individualize experience moral?
	- amazon adjusting prices,
	- insurance companies not accepting 
	- ect. 

### Flipside

- if privacy-preserving techniques ultimately fail, 
	- then what?
	
- ML does a lot of good
	- though the individual might not want to share medical data,
	- that same data on the large scale could save countless lives

- garbage in, garbage out + black box
	- not. good. 
	- especially when AI will dominate decision making and analysis.

- forced to give data by tragedy of the commons
	- you want ml networks to portray your demographic accurately,
	- and if you don't give data, it won't. In a world dominated by ml,
	- giving your data becomes necessity

- forced to give data by economic necessity
	- where data collection is truly opt out, services replace it with paid
	- economically worse off will be more likely to give data
	- skews data collected? 

















